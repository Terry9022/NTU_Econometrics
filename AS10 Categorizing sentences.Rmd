---
title: "AS#10 Categorizing sentences"
author: "B06109022 呂紀廷"
date: "12/22/2020"
output:
  html_document:
    highlight: zenburn
    number_sections: yes
    theme: cerulean
    toc: yes
    css: style.css
editor_options:
  chunk_output_type: inline
---

# Install package
```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
options(stringsAsFactors = F)
options(verbose = T)
options(scipen = 999)
library(stringr)
library(tidytext) # unnest() unnest_tokens()
library(jiebaR)
library(lubridate)
```

# Loading data
- Mutate sentence_id
- Segmenting text to sentence

```{r}
raw <- read_csv("data/hackathon/task1_trainset.csv") %>%
    slice(1:4000) %>%
    mutate(sentence  = str_split(Abstract, "\\$+"),
           sentence_type = str_split(`Task 1`, " ")) %>%
    unnest(sentence, sentence_type) %>%
    filter(!str_detect(sentence_type, "/")) %>%
    # mutate(sentence_type = str_split(sentence_type, "/")) %>%
    # unnest(sentence_type) %>%
    select(doc_id = Id, everything()) %>%
    group_by(doc_id) %>%
    mutate(sentence_id = str_c(doc_id, "_", row_number())) %>%
    mutate(sentence_perc = row_number()/n()) %>%
    ungroup() %>%
    mutate(num_count = str_count(sentence, "\\d+")) %>%
    mutate(nword = str_count(sentence, "\\s+")) %>%
    mutate(comma_count = str_count(sentence, ",")) %>%
    mutate(semicolon_count = str_count(sentence, ";")) %>%
    select(-`Task 1`, -Abstract) %>%
    # filter(!sentence_type %in% c("OTHERS")) %>%
    mutate(sentence_type = as.factor(sentence_type))

raw %>% count(sentence_type)
raw %>% glimpse()

```


# Word Feature selections


## stop_words as features

將停用字拿來作為Feature selections

```{r}
doc_word_count <- raw %>%
    select(sentence_id, sentence) %>%
    unnest_tokens(word, sentence, token = "regex", pattern = "[^A-Za-z\\d#@']") %>%
    filter(word %in% stop_words$word) %>%
    # count(word, sort = T) %>% View
    group_by(word) %>%
    filter(n() > 20 & n() < 2000) %>%
    ungroup() %>%
    filter(!word %in% c("in", "a", "to", "and", "for", "that", "is", "on", "with", "are", "by", "an", "be")) %>%
    count(sentence_id, word) %>%
    bind_tf_idf(word, sentence_id, n)

message("Number of words: ", unique(doc_word_count$word) %>% length)
```



# Building dtm

建立字詞向量
```{r}

dtm <- doc_word_count %>% 
    cast_dtm(document = sentence_id, term = word, value = tf)
dtm %>% dim

mat.df <- as.matrix(dtm) %>% as_tibble() %>% 
    bind_cols(sentence_id = dtm$dimnames$Docs) %>%
    left_join(raw %>%
                  select(sentence_id, sentence_type, 
                         sentence_perc, num_count, comma_count) 
              # %>% filter(!duplicated(sentence_id, sentence_type))
              ) 
colnames(mat.df) <- make.names(colnames(mat.df))
```



# Dividing to test and training set
```{r}
index <- sample(1:nrow(mat.df), ceiling(nrow(mat.df) * .70))

train.df <- mat.df[index, ]
test.df <- mat.df[-index, ]

dim(train.df)
dim(test.df)
```



# Modeling


## multinomial regression

```{r}
library(nnet)

predicted <- test.df %>%
    select(sentence_id, sentence_type)


stime <- Sys.time() 
fit_mnl <- multinom(sentence_type ~ ., data = train.df %>% select(-sentence_id), MaxNWts = 10000, maxit=100)
ttime <- Sys.time(); str_c("t(training): ", ttime - stime)
predicted$mnl <- predict(fit_mnl, newdata = test.df %>% select(-sentence_id), "class")
str_c("t(predicting): ", Sys.time() - ttime)


```



```{r}
library(mltest)

classifier_metrics <- ml_test(predicted$mnl, predicted$sentence_type, output.as.table = FALSE)

#accuracy
classifier_metrics$accuracy

#precision
classifier_metrics$precision

#recall
classifier_metrics$recall

#F1
classifier_metrics$F1


# Micro F1
#install.packages("yardstick")
library(yardstick)

microF1<-f_meas(predicted, predicted$sentence_type, predicted$mnl, estimator = "micro")
microF1
```

使用multinomial regression來當作分類器，準確度有 54.9% ， Micro F1 為 54.9%

